# 100daysMLchallenge

Learning something new with ML each day. The motive behind this repo is to spend 30 mins or more in an ML problem and learn something new. 

# Day 1
## 6 January 
> Dataset : Medical cost personal dataset

> **Note:  Logs**
> - Using Logarithms: Logarithms helps us have a normal distribution which could help us in a number of different ways such as outlier detection, implementation of statistical concepts based on the central limit theorem and for our predictive modell in the foreseen future. 

# Day 2
## 7 January 
> Dataset : Medical cost personal dataset

> **Note:  OLS and Hyppthesis Testing**
> - OLS is also known as the Ordinary Least Squares. It is a method to find a linear relation between the independent variable and dependent variable. 
> - In stats module ols is can be used to find the p value between the two. The p-value or the probability value is used to test the hypothesis against the alternative hypothesis which stands against the ruling hypothesis or the null hypothesis. 
> - The alternative hypothesis is the one you would believe if the null hypothesis is concluded to be untrue. The evidence in the trial is your data and the statistics that go along with it. All hypothesis tests ultimately use a p-value to weigh the strength of the evidence (what the data are telling you about the population). The p-value is a number between 0 and 1 and interpreted in the following way:

>> 1. A small p-value (typically â‰¤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.
>> 2. A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.
>> 3. p-values very close to the cutoff (0.05) are considered to be marginal (could go either way). Always report the p-value so your readers can draw their own conclusions.

# Day 3
## 8 January 
> Dataset : Medical cost personal dataset

> **Note: Learning Data Exploration**
> - Learning to explore correlation and colinearity and also reason out the plausible conclusion as how and why a particular feature is important. 

# Day 4
## 13 January 
> Dataset : Medical cost personal dataset

> **Note: Intro to Unsupervised Learning**
> - Using clusterring to understand the distribution of data and make sense of it. 
> - The next two days will learning and understanding:
>>  - K-means clusterring 
>>  - Hierarchical clusterring


# Day 5
## 14 January 
> Dataset : Medical cost personal dataset

> **Note: Intro to Unsupervised Learning -- KMeans**
> - In the quick half an hour session that I got to learn something about K-means clusterring is that it find groups or cluster based upon the centroid which is quite un-reliable which I though in the beginning but later on as I started to look more into the centroid patterns started to emerge. For example in manual clusterring there was no distinct geography on which the data is being seperated it is just based upon the fact that some data share a particular property where as others do not. In case of KMeans we see a distinct geography seperating the two region or the clusters. KMeans serves as opportunity to segregate the based upon the gravity of the center also known as the centroid. An emerging cluster finds its way by creating different centriods and then reducing the error to reach the center of the cluster. 

